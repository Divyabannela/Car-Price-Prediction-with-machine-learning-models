# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19XgLygMq40kdX6ep_e79PeAB-6R55xEy

Required Libraries
"""

import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

"""Loading the Dataset"""

data = pd.read_csv("/content/Car_Prices.csv")

data.head(8)

"""Data Processing

Handling missing Values
"""

missing_values = data.isnull().sum().sum()

if missing_values > 0:
    print("The dataset has missing values.")
    print(f"Total Missing Values: {missing_values}")
    print("Columns with Missing Values:")
    print(data.isnull().sum())
else:
    print("The dataset has no missing values.")

"""Treating Outliers"""

for column in data.select_dtypes(include=['float64']).columns:
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    data = data[~((data[column] < (Q1 - 1.5 * IQR)) | (data[column] > (Q3 + 1.5 * IQR)))]

"""Encoding Categorical Variables"""

data_encoded = pd.get_dummies(data, drop_first=True)

print(data.columns)

"""Feature Engineering"""

data['CarVolume'] = data['carlength'] * data['carwidth'] * data['carheight']

"""Exploratory Data Analysis

EDA: Distribution of Car Prices
"""

plt.figure(figsize=(10, 6))
sns.histplot(data['price'], bins=30, kde=True)
plt.title('Distribution of Car Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

"""EDA: Correlation Matrix"""

correlation_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""EDA: Pairplot for selected features"""

selected_features = ['wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize', 'horsepower', 'price']
sns.pairplot(data[selected_features])
plt.suptitle('Pairplot of Selected Features', y=1.02)
plt.show()

"""Summary statistics"""

print(data.describe())

"""Model Building and Evaluation"""

# Assuming "price" is the target variable
X = data_encoded.drop("price", axis=1)
y = data_encoded["price"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train, y_train)

y_pred_linear_reg = linear_reg_model.predict(X_test)

# Random Forest
random_forest_model = RandomForestRegressor()
random_forest_model.fit(X_train, y_train)

y_pred_random_forest = random_forest_model.predict(X_test)

# Decision Tree
decision_tree_model = DecisionTreeRegressor()
decision_tree_model.fit(X_train, y_train)

y_pred_decision_tree = decision_tree_model.predict(X_test)

# gradient boosting regression
gradient_boosting_reg_model = GradientBoostingRegressor()
gradient_boosting_reg_model.fit(X_train, y_train)

y_pred = gb_model.predict(X_test)

# Evaluation of Linear Regression
mse_linear_reg = mean_squared_error(y_test, y_pred_linear_reg)
rmse_linear_reg = mean_squared_error(y_test, y_pred_linear_reg, squared=False)
r2_linear_reg = r2_score(y_test, y_pred_linear_reg)

print("Linear Regression Metrics:")
print(f"Mean Squared Error: {mse_linear_reg}")
print(f"Root Mean Squared Error: {rmse_linear_reg}")
print(f"R-squared: {r2_linear_reg}")
print()

# Evaluation of Random Forest
mse_random_forest = mean_squared_error(y_test, y_pred_random_forest)
rmse_random_forest = mean_squared_error(y_test, y_pred_random_forest, squared=False)
r2_random_forest = r2_score(y_test, y_pred_random_forest)

print("Random Forest Metrics:")
print(f"Mean Squared Error: {mse_random_forest}")
print(f"Root Mean Squared Error: {rmse_random_forest}")
print(f"R-squared: {r2_random_forest}")
print()

# Evaluation of Decision Tree
mse_decision_tree = mean_squared_error(y_test, y_pred_decision_tree)
rmse_decision_tree = mean_squared_error(y_test, y_pred_decision_tree, squared=False)
r2_decision_tree = r2_score(y_test, y_pred_decision_tree)

print("Decision Tree Metrics:")
print(f"Mean Squared Error: {mse_decision_tree}")
print(f"Root Mean Squared Error: {rmse_decision_tree}")
print(f"R-squared: {r2_decision_tree}")
print()

# Evaluation of gradient boosting regression
mse_gradient_boosting_reg = mean_squared_error(y_test, y_pred)
rmse_gradient_boosting_reg = np.sqrt(mse)
r2_gradient_boosting_reg = r2_score(y_test, y_pred)

print("Gradient Boosting Regression Metrics:")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

"""Tuning"""

# Fine-tune Linear Regression hyperparameters using GridSearchCV
param_grid_lr = {'fit_intercept': [True, False], 'copy_X': [True, False]}
grid_search_lr = GridSearchCV(LinearRegression(), param_grid_lr, cv=5, scoring='neg_mean_squared_error')
grid_search_lr.fit(X_train, y_train)

best_lr_model = grid_search_lr.best_estimator_

# Fine-tune Random Forest hyperparameters using GridSearchCV
param_grid_rf = {'n_estimators': [50, 100, 200],
                 'max_depth': [None, 10, 20, 30],
                 'min_samples_split': [2, 5, 10],
                 'min_samples_leaf': [1, 2, 4]}
grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5, scoring='neg_mean_squared_error')
grid_search_rf.fit(X_train[:1000], y_train[:1000])

best_rf_model = grid_search_rf.best_estimator_

# Fine-tune Decision Tree hyperparameters using GridSearchCV
param_grid_dt = {'max_depth': [None, 10, 20, 30],
                 'min_samples_split': [2, 5, 10],
                 'min_samples_leaf': [1, 2, 4]}
grid_search_dt = GridSearchCV(DecisionTreeRegressor(), param_grid_dt, cv=5, scoring='neg_mean_squared_error')
grid_search_dt.fit(X_train, y_train)

best_dt_model = grid_search_dt.best_estimator_

# Fine-tune Gradient Boosting Regression hyperparameters using GridSearchCV
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 0.9, 1.0]
}
grid_search_gb = GridSearchCV(gb_model, param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_gb.fit(X_train, y_train)

best_gb_model = grid_search_gb.best_estimator_


# Evaluate the best models on the test set
y_pred_lr = best_lr_model.predict(X_test)
y_pred_rf = best_rf_model.predict(X_test)
y_pred_dt = best_dt_model.predict(X_test)
y_pred_gb = best_gb_model.predicr(X_test)

mse_lr = mean_squared_error(y_test, y_pred_lr)
mse_rf = mean_squared_error(y_test, y_pred_rf)
mse_dt = mean_squared_error(y_test, y_pred_dt)
mse_gb = mean_squared_error(y_test, y_pred_gb)

print("Linear Regression MSE:", mse_lr)
print("Random Forest MSE:", mse_rf)
print("Decision Tree MSE:", mse_dt)
print("Gradient Boosting Regression MSE:", mse_gb)

# You can access the best hyperparameters using best_estimator_.get_params()
print("\nBest Linear Regression Model Hyperparameters:")
print(best_lr_model.get_params())

print("\nBest Random Forest Model Hyperparameters:")
print(best_rf_model.get_params())

print("Best Gradient Boosting Regression Model Hyperparameters:")
print(grid_search_gb.best_params_)

"""Model Interpretation"""

# Interpret Linear Regression
coefficients = best_lr_model.coef_
lr_feature_importance = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})

# Interpret Random Forest
rf_importances = best_rf_model.feature_importances_
rf_feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance_RF': rf_importances})

# Interpret Decision Tree
dt_importances = best_dt_model.feature_importances_
dt_feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance_DT': dt_importances})

# Interpret Gradient Boosting Regression
gb_importances = best_gb_model.feature_importances_
gb_feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance_GB': gb_importances})

# Combine the results into one DataFrame
feature_importances_combined_df = pd.merge(lr_feature_importance, rf_feature_importance, on='Feature')
feature_importances_combined_df = pd.merge(feature_importances_combined_df, dt_feature_importance, on='Feature')
feature_importances_combined_df = pd.merge(feature_importances_combined_df, gb_feature_importance, on='Feature')


# Display the combined feature importance
print("Combined Feature Importance:")
print(feature_importances_combined_df)

# Visualization: Bar plots for comparison
plt.figure(figsize=(15, 6))

plt.subplot(131)
plt.bar(lr_feature_importance['Feature'], lr_feature_importance['Coefficient'])
plt.title('Linear Regression Feature Coefficients')
plt.xticks(rotation=90)

plt.subplot(132)
plt.bar(rf_feature_importance['Feature'], rf_feature_importance['Importance_RF'])
plt.title('Random Forest Feature Importances')
plt.xticks(rotation=90)

plt.subplot(133)
plt.bar(dt_feature_importance['Feature'], dt_feature_importance['Importance_DT'])
plt.title('Decision Tree Feature Importances')
plt.xticks(rotation=90)

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 6))
plt.subplot(131)
plt.bar(gb_feature_importance['Feature'], gb_feature_importance['Importance_GB'])
plt.title('Gradient Boosting regression Feature Importances')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Calculate a new column for combined absolute importance
feature_importances_combined_df['Combined_Absolute_Importance'] = (
    abs(feature_importances_combined_df['Coefficient']) +
    abs(feature_importances_combined_df['Importance_RF']) +
    abs(feature_importances_combined_df['Importance_DT']) +
    abs(feature_importances_combined_df['Importance_GB'])
)

# Display the top N features with the highest combined absolute impact
top_features = feature_importances_combined_df.sort_values(by='Combined_Absolute_Importance',ascending=False).head(10)

# Display the top features
print("Top Features with the Most Significant Impact:")
print(top_features)